{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions/to-do\n",
    "\n",
    "Check whether stemming & lemmatization make difference\n",
    "Do we need bigram/trigram frequencies (within titles)?\n",
    "Do we need dataframe including cleaned data & genres? (which cleaned data?)\n",
    "How do we incorporate POS-tagging & NER in classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Cleaning data\n",
    "\n",
    "This section imports the data into a pandas dataframe and goes through the following preprocessing steps:\n",
    "-Case collapsing\n",
    "-Remove punctuation\n",
    "-Tokenization\n",
    "-N-Grams: bigrams and trigrams\n",
    "-Stemming -> check whether this makes a difference\n",
    "-Lemmatization -> check whether this makes a difference\n",
    "-Part-of-speech (POS) tagging\n",
    "-Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Imports data as pandas dataframe\n",
    "Create list of title and genre columns from original data\n",
    "Create list including all 32 genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title      genre\n",
      "0                         Mom's Family Wall Calendar 2016  Calendars\n",
      "1                         Doug the Pug 2016 Wall Calendar  Calendars\n",
      "2       Moleskine 2016 Weekly Notebook, 12M, Large, Bl...  Calendars\n",
      "3                 365 Cats Color Page-A-Day Calendar 2016  Calendars\n",
      "4                    Sierra Club Engagement Calendar 2016  Calendars\n",
      "...                                                   ...        ...\n",
      "207567  ADC the Map People Washington D.C.: Street Map...     Travel\n",
      "207568  Washington, D.C., Then and Now: 69 Sites Photo...     Travel\n",
      "207569  The Unofficial Guide to Washington, D.C. (Unof...     Travel\n",
      "207570      Washington, D.C. For Dummies (Dummies Travel)     Travel\n",
      "207571  Fodor's Where to Weekend Around Boston, 1st Ed...     Travel\n",
      "\n",
      "[207572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/feliciaheilgendorff/Documents/AU/NLP/NLP-Project/amazon/book32listing.csv', encoding='latin1', header=None)\n",
    "df1 = df[[3,6]] # only columns with titles and genres\n",
    "df1.columns = ['title', 'genre']\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Mom's Family Wall Calendar 2016\", 'Doug the Pug 2016 Wall Calendar', 'Moleskine 2016 Weekly Notebook, 12M, Large, Black, Soft Cover (5 x 8.25)', '365 Cats Color Page-A-Day Calendar 2016', 'Sierra Club Engagement Calendar 2016', 'Sierra Club Wilderness Calendar 2016']\n"
     ]
    }
   ],
   "source": [
    "titles = df1['title'] # list of all titles\n",
    "titles1 = titles.values.tolist() # change to list of strings\n",
    "print(titles1[0:6]) # test whether it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arts & Photography', 'Biographies & Memoirs', 'Business & Money', 'Calendars', 'Childrens Books', 'Comics & Graphic Novels', 'Computers & Technology', 'Cookbooks, Food & Wine', 'Crafts, Hobbies & Home', 'Christian Books & Bibles', 'Engineering & Transportation', 'Health, Fitness & Dieting', 'History', 'Humor & Entertainment', 'Law', 'Literature & Fiction', 'Medical Books', 'Mystery, Thriller & Suspense', 'Parenting & Relationships', 'Politics & Social Sciences', 'Reference', 'Religion & Spirituality', 'Romance', 'Science & Math', 'Science Fiction & Fantasy', 'Self-Help', 'Sports & Outdoors', 'Teen & Young Adult', 'Test Preparation', 'Travel', 'Gay & Lesbian', 'Education & Teaching']\n"
     ]
    }
   ],
   "source": [
    "genres = ['Arts & Photography', 'Biographies & Memoirs', \n",
    "'Business & Money', 'Calendars', 'Children''s Books', 'Comics & Graphic Novels', \n",
    "'Computers & Technology', 'Cookbooks, Food & Wine', 'Crafts, Hobbies & Home', \n",
    "'Christian Books & Bibles', 'Engineering & Transportation', 'Health, Fitness & Dieting', \n",
    "'History', 'Humor & Entertainment', 'Law', 'Literature & Fiction', 'Medical Books', \n",
    "'Mystery, Thriller & Suspense', 'Parenting & Relationships', 'Politics & Social Sciences', \n",
    "'Reference', 'Religion & Spirituality', 'Romance', 'Science & Math', 'Science Fiction & Fantasy', \n",
    "'Self-Help', 'Sports & Outdoors', 'Teen & Young Adult', 'Test Preparation', 'Travel', \n",
    "'Gay & Lesbian', 'Education & Teaching']\n",
    "print(genres) # list of all possible genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Collapsing\n",
    "Change all uppercase to lowercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"mom's family wall calendar 2016\", 'doug the pug 2016 wall calendar', 'moleskine 2016 weekly notebook, 12m, large, black, soft cover (5 x 8.25)', '365 cats color page-a-day calendar 2016', 'sierra club engagement calendar 2016', 'sierra club wilderness calendar 2016']\n"
     ]
    }
   ],
   "source": [
    "case_collap = map(lambda x:x.lower(), titles1)\n",
    "case_collap_list = list(case_collap)\n",
    "print(case_collap_list[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "Remove punctuation by creating translation table\n",
    "Punctuation to be removed is given in string: string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moms family wall calendar 2016', 'doug the pug 2016 wall calendar', 'moleskine 2016 weekly notebook 12m large black soft cover 5 x 825', '365 cats color pageaday calendar 2016', 'sierra club engagement calendar 2016', 'sierra club wilderness calendar 2016']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "trans = str.maketrans('', '', string.punctuation)\n",
    "rem_punct = [s.translate(trans) for s in case_collap_list]\n",
    "print(rem_punct[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Split all titles into words\n",
    "Output: list of lists of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['moms', 'family', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskine', '2016', 'weekly', 'notebook', '12m', 'large', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cats', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engagement', 'calendar', '2016'], ['sierra', 'club', 'wilderness', 'calendar', '2016'], ['thomas', 'kinkade', 'the', 'disney', 'dreams', 'collection', '2016', 'wall', 'calendar'], ['ansel', 'adams', '2016', 'wall', 'calendar'], ['dilbert', '2016', 'daytoday', 'calendar'], ['mary', 'engelbreit', '2016', 'deluxe', 'wall', 'calendar', 'never', 'give', 'up']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_titles = [word_tokenize(i) for i in rem_punct]\n",
    "print(tokenized_titles[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Stopwords\n",
    "Do we need to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams: Bigrams and Trigrams\n",
    "Create bigrams + trigrams\n",
    "(could be done in one go e.g. n=2 for bigrams, n=3 for trigrams etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['moms', 'family'], ['family', 'wall'], ['wall', 'calendar'], ['calendar', '2016']], [['doug', 'the'], ['the', 'pug'], ['pug', '2016'], ['2016', 'wall'], ['wall', 'calendar']], [['moleskine', '2016'], ['2016', 'weekly'], ['weekly', 'notebook'], ['notebook', '12m'], ['12m', 'large'], ['large', 'black'], ['black', 'soft'], ['soft', 'cover'], ['cover', '5'], ['5', 'x'], ['x', '825']], [['365', 'cats'], ['cats', 'color'], ['color', 'pageaday'], ['pageaday', 'calendar'], ['calendar', '2016']], [['sierra', 'club'], ['club', 'engagement'], ['engagement', 'calendar'], ['calendar', '2016']], [['sierra', 'club'], ['club', 'wilderness'], ['wilderness', 'calendar'], ['calendar', '2016']]]\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "token_bigram = []\n",
    "for title in tokenized_titles:\n",
    "    title_bigram = []\n",
    "    for w in range(len(title) - 1):\n",
    "        title_bigram.append([title[w], title[w + 1]])\n",
    "    token_bigram.append(title_bigram)\n",
    "print(token_bigram[:6]) # test whether working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['moms', 'family', 'wall'], ['family', 'wall', 'calendar'], ['wall', 'calendar', '2016']], [['doug', 'the', 'pug'], ['the', 'pug', '2016'], ['pug', '2016', 'wall'], ['2016', 'wall', 'calendar']], [['moleskine', '2016', 'weekly'], ['2016', 'weekly', 'notebook'], ['weekly', 'notebook', '12m'], ['notebook', '12m', 'large'], ['12m', 'large', 'black'], ['large', 'black', 'soft'], ['black', 'soft', 'cover'], ['soft', 'cover', '5'], ['cover', '5', 'x'], ['5', 'x', '825']], [['365', 'cats', 'color'], ['cats', 'color', 'pageaday'], ['color', 'pageaday', 'calendar'], ['pageaday', 'calendar', '2016']], [['sierra', 'club', 'engagement'], ['club', 'engagement', 'calendar'], ['engagement', 'calendar', '2016']], [['sierra', 'club', 'wilderness'], ['club', 'wilderness', 'calendar'], ['wilderness', 'calendar', '2016']]]\n"
     ]
    }
   ],
   "source": [
    "# trigrams\n",
    "token_trigram = []\n",
    "for title in tokenized_titles:\n",
    "    title_trigram = []\n",
    "    for w in range(len(title) - 2):\n",
    "        title_trigram.append([title[w], title[w + 1], title[w + 2]])\n",
    "    token_trigram.append(title_trigram)\n",
    "print(token_trigram[:6]) # test whether working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Test this out to see whether it makes a difference in final classifier\n",
    "\n",
    "PorterStemmer (one algorithm for stemming; less aggressive than LancasterStemming)\n",
    "\n",
    "Create empty list to contain lists of stems in each title\n",
    "Create empty list for stems of title\n",
    "Add each stemmed word in title to second list\n",
    "Append list of stemmed words to first list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mom', 'famili', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskin', '2016', 'weekli', 'notebook', '12m', 'larg', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cat', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engag', 'calendar', '2016'], ['sierra', 'club', 'wilder', 'calendar', '2016']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stems = []   \n",
    "for title in tokenized_titles:\n",
    "    stems_title = []\n",
    "    for word in title:\n",
    "        stems_title.append(porter.stem(word))\n",
    "    stems.append(stems_title)\n",
    "    \n",
    "print(stems[0:6]) # test whether it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Same principle as with stemming\n",
    "Test this out to see whether it makes a difference in final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/feliciaheilgendorff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mom', 'family', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskine', '2016', 'weekly', 'notebook', '12m', 'large', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cat', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engagement', 'calendar', '2016'], ['sierra', 'club', 'wilderness', 'calendar', '2016']]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = []\n",
    "for title in tokenized_titles:\n",
    "    lemmas_title = []\n",
    "    for word in title:\n",
    "        lemmas_title.append(lemmatizer.lemmatize(word))\n",
    "    lemmas.append(lemmas_title)\n",
    "    \n",
    "print(lemmas[0:6]) # test whether it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "Create list of lists with tokens and their corresponding part-of-speech tag in each title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/feliciaheilgendorff/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('moms', 'NNS'), ('family', 'NN'), ('wall', 'NN'), ('calendar', 'NN'), ('2016', 'CD')], [('doug', 'VB'), ('the', 'DT'), ('pug', 'NN'), ('2016', 'CD'), ('wall', 'NN'), ('calendar', 'NN')], [('moleskine', 'NN'), ('2016', 'CD'), ('weekly', 'JJ'), ('notebook', 'NN'), ('12m', 'CD'), ('large', 'JJ'), ('black', 'JJ'), ('soft', 'JJ'), ('cover', 'NN'), ('5', 'CD'), ('x', 'JJ'), ('825', 'CD')], [('365', 'CD'), ('cats', 'NNS'), ('color', 'VBP'), ('pageaday', 'IN'), ('calendar', 'NN'), ('2016', 'CD')], [('sierra', 'NN'), ('club', 'NN'), ('engagement', 'NN'), ('calendar', 'NN'), ('2016', 'CD')], [('sierra', 'NN'), ('club', 'NN'), ('wilderness', 'NN'), ('calendar', 'NN'), ('2016', 'CD')]]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "postag = []\n",
    "for title in tokenized_titles:\n",
    "    postag.append(nltk.pos_tag(title))\n",
    "    \n",
    "print(postag[0:6]) # testing whether postag worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "Create list of lists with words, correspondent POS and named entity tags for each title\n",
    "This uses postags created in previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/feliciaheilgendorff/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/feliciaheilgendorff/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Mom', 'NNP', 'B-PERSON'), (\"'s\", 'POS', 'O'), ('Family', 'NNP', 'B-PERSON'), ('Wall', 'NNP', 'I-PERSON'), ('Calendar', 'NNP', 'I-PERSON'), ('2016', 'CD', 'O')], [('Doug', 'NNP', 'O'), ('the', 'DT', 'O'), ('Pug', 'NNP', 'O'), ('2016', 'CD', 'O'), ('Wall', 'NNP', 'B-FACILITY'), ('Calendar', 'NNP', 'I-FACILITY')], [('Moleskine', 'NN', 'O'), ('2016', 'CD', 'O'), ('Weekly', 'NNP', 'O'), ('Notebook', 'NNP', 'O'), (',', ',', 'O'), ('12M', 'CD', 'O'), (',', ',', 'O'), ('Large', 'NNP', 'B-PERSON'), (',', ',', 'O'), ('Black', 'NNP', 'B-PERSON'), (',', ',', 'O'), ('Soft', 'NNP', 'B-PERSON'), ('Cover', 'NNP', 'I-PERSON'), ('(', '(', 'O'), ('5', 'CD', 'O'), ('x', 'RB', 'O'), ('8.25', 'CD', 'O'), (')', ')', 'O')], [('365', 'CD', 'O'), ('Cats', 'NNPS', 'B-ORGANIZATION'), ('Color', 'NNP', 'I-ORGANIZATION'), ('Page-A-Day', 'NNP', 'O'), ('Calendar', 'NNP', 'O'), ('2016', 'CD', 'O')], [('Sierra', 'NNP', 'B-PERSON'), ('Club', 'NNP', 'B-ORGANIZATION'), ('Engagement', 'NNP', 'I-ORGANIZATION'), ('Calendar', 'NNP', 'O'), ('2016', 'CD', 'O')], [('Sierra', 'NNP', 'B-PERSON'), ('Club', 'NNP', 'B-ORGANIZATION'), ('Wilderness', 'NNP', 'I-ORGANIZATION'), ('Calendar', 'NNP', 'O'), ('2016', 'CD', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "ner = []\n",
    "for title in titles1:\n",
    "    ner.append(tree2conlltags(ne_chunk(pos_tag(word_tokenize(title)))))\n",
    "    \n",
    "print(ner[0:6]) # test whether NER worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "Check whether stemming/lemmatization make difference in final classifier\n",
    "-> Does it improve/worsen classifier?\n",
    "\n",
    "Naive Bayes, BERT, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "split into training and test data -> 80% / 20%\n",
    "use n-grams ?\n",
    "\n",
    "build vocabulary -> need token frequencies across all titles (create sublist with tokenized_titles)\n",
    "vocabulary for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'moms': 1, 'family': 1, 'wall': 1, 'calendar': 1, '2016': 1}), Counter({'doug': 1, 'the': 1, 'pug': 1, '2016': 1, 'wall': 1, 'calendar': 1}), Counter({'moleskine': 1, '2016': 1, 'weekly': 1, 'notebook': 1, '12m': 1, 'large': 1, 'black': 1, 'soft': 1, 'cover': 1, '5': 1, 'x': 1, '825': 1}), Counter({'365': 1, 'cats': 1, 'color': 1, 'pageaday': 1, 'calendar': 1, '2016': 1}), Counter({'sierra': 1, 'club': 1, 'engagement': 1, 'calendar': 1, '2016': 1}), Counter({'sierra': 1, 'club': 1, 'wilderness': 1, 'calendar': 1, '2016': 1}), Counter({'thomas': 1, 'kinkade': 1, 'the': 1, 'disney': 1, 'dreams': 1, 'collection': 1, '2016': 1, 'wall': 1, 'calendar': 1}), Counter({'ansel': 1, 'adams': 1, '2016': 1, 'wall': 1, 'calendar': 1}), Counter({'dilbert': 1, '2016': 1, 'daytoday': 1, 'calendar': 1}), Counter({'mary': 1, 'engelbreit': 1, '2016': 1, 'deluxe': 1, 'wall': 1, 'calendar': 1, 'never': 1, 'give': 1, 'up': 1}), Counter({'cat': 1, 'pageaday': 1, 'gallery': 1, 'calendar': 1, '2016': 1}), Counter({'llewellyns': 1, '2016': 1, 'witches': 1, 'datebook': 1}), Counter({'the': 2, '2016': 1, 'amy': 1, 'knapp': 1, 'big': 1, 'grid': 1, 'wall': 1, 'calendar': 1, 'essential': 1, 'organization': 1, 'and': 1, 'communication': 1, 'tool': 1, 'for': 1, 'entire': 1, 'family': 1}), Counter({'outlander': 1, '2016': 1, 'wall': 1, 'calendar': 1}), Counter({'audubon': 1, 'nature': 1, 'wall': 1, 'calendar': 1, '2016': 1}), Counter({'2016': 1, 'national': 1, 'park': 1, 'foundation': 1, 'wall': 1, 'calendar': 1}), Counter({'color': 1, 'your': 1, 'year': 1, 'wall': 1, 'calendar': 1, '2016': 1, 'mindful': 1, 'coloring': 1, 'through': 1, 'the': 1, 'seasons': 1}), Counter({'mary': 1, 'engelbreit': 1, '2016': 1, 'daytoday': 1, 'calendar': 1, 'enjoy': 1, 'the': 1, 'joy': 1}), Counter({'grumpy': 1, 'cat': 1, '2016': 1, 'wall': 1, 'calendar': 1}), Counter({'moleskine': 1, '2016': 1, 'weekly': 1, 'notebook': 1, '12m': 1, 'extra': 1, 'large': 1, 'black': 1, 'soft': 1, 'cover': 1, '75': 1, 'x': 1, '10': 1})]\n"
     ]
    }
   ],
   "source": [
    "# token frequencies within each title\n",
    "from collections import Counter\n",
    "\n",
    "unigram_count = []\n",
    "for title in tokenized_titles:\n",
    "    uni_title = Counter()\n",
    "    for i in title:\n",
    "        uni_title[i] += 1\n",
    "    unigram_count.append(uni_title)\n",
    "\n",
    "print(unigram_count[:20]) # test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python37964bitnlpcondaede2030205ae4e968bf0733df961d627"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
