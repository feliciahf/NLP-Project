{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('nlp': conda)",
      "language": "python",
      "name": "python37964bitnlpcondaede2030205ae4e968bf0733df961d627"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Project_FH.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feliciahf/NLP-Project/blob/main/amazon/colab_FH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldZdXc3eW2-7"
      },
      "source": [
        "# Notes\n",
        "\n",
        "Check whether stemming & lemmatization make difference\n",
        "Do we need bigram/trigram frequencies (within titles)?\n",
        "Do we need dataframe including cleaned data & genres? (which cleaned data?) -> create dictionary of lists maybe\n",
        "https://www.geeksforgeeks.org/python-ways-to-create-a-dictionary-of-lists/\n",
        "How do we incorporate POS-tagging & NER in classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzUx0mrfW2-7"
      },
      "source": [
        "# Importing & Cleaning data\n",
        "\n",
        "This section imports the data into a pandas dataframe and goes through the following preprocessing steps:\n",
        "-Case collapsing\n",
        "-Remove punctuation\n",
        "-Tokenization\n",
        "-N-Grams: bigrams and trigrams\n",
        "-Stemming -> check whether this makes a difference\n",
        "-Lemmatization -> check whether this makes a difference\n",
        "-Part-of-speech (POS) tagging\n",
        "-Named entity recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mpLoYwJW2-7"
      },
      "source": [
        "## Import Data\n",
        "Imports data as pandas dataframe\n",
        "Create list of title and genre columns from original data\n",
        "Create list including all 32 genres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7qgLdcrYdZI",
        "outputId": "a4376a7c-47e2-4fd0-cd30-3d607be5d974",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e219b433-bf13-4527-add9-325929d3bc42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e219b433-bf13-4527-add9-325929d3bc42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving book32listing.csv to book32listing.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ygCh_72W2-7",
        "outputId": "065751b3-a4d4-4335-9c6e-e057ece0e523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"book32listing.csv\", encoding='latin1', header=None)\n",
        "df1 = df[[3,6]] # only columns with titles and genres\n",
        "df1.columns = ['title', 'genre']\n",
        "print(df1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    title      genre\n",
            "0                         Mom's Family Wall Calendar 2016  Calendars\n",
            "1                         Doug the Pug 2016 Wall Calendar  Calendars\n",
            "2       Moleskine 2016 Weekly Notebook, 12M, Large, Bl...  Calendars\n",
            "3                 365 Cats Color Page-A-Day Calendar 2016  Calendars\n",
            "4                    Sierra Club Engagement Calendar 2016  Calendars\n",
            "...                                                   ...        ...\n",
            "207567  ADC the Map People Washington D.C.: Street Map...     Travel\n",
            "207568  Washington, D.C., Then and Now: 69 Sites Photo...     Travel\n",
            "207569  The Unofficial Guide to Washington, D.C. (Unof...     Travel\n",
            "207570      Washington, D.C. For Dummies (Dummies Travel)     Travel\n",
            "207571  Fodor's Where to Weekend Around Boston, 1st Ed...     Travel\n",
            "\n",
            "[207572 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fvuq6lyW2-9",
        "outputId": "e2e7182a-03bd-4c3e-e876-435ff399fe7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "titles = df1['title'] # list of all titles\n",
        "titles1 = titles.values.tolist() # change to list of strings\n",
        "print(titles1[0:6]) # test whether it worked"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Mom's Family Wall Calendar 2016\", 'Doug the Pug 2016 Wall Calendar', 'Moleskine 2016 Weekly Notebook, 12M, Large, Black, Soft Cover (5 x 8.25)', '365 Cats Color Page-A-Day Calendar 2016', 'Sierra Club Engagement Calendar 2016', 'Sierra Club Wilderness Calendar 2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct2JeW8FW2--"
      },
      "source": [
        "genres = df1['genre']\n",
        "genres = genres.values.tolist()\n",
        "genres = pd.DataFrame(genres)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfotFMqcxbw",
        "outputId": "8dff4f26-ec73-4474-cbad-717707dffb4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1.genre.unique() # list of all possible genres"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Calendars', 'Comics & Graphic Novels', 'Test Preparation',\n",
              "       'Mystery, Thriller & Suspense', 'Science Fiction & Fantasy',\n",
              "       'Romance', 'Humor & Entertainment', 'Literature & Fiction',\n",
              "       'Gay & Lesbian', 'Engineering & Transportation',\n",
              "       'Cookbooks, Food & Wine', 'Crafts, Hobbies & Home',\n",
              "       'Arts & Photography', 'Education & Teaching',\n",
              "       'Parenting & Relationships', 'Self-Help', 'Computers & Technology',\n",
              "       'Medical Books', 'Science & Math', 'Health, Fitness & Dieting',\n",
              "       'Business & Money', 'Law', 'Biographies & Memoirs', 'History',\n",
              "       'Politics & Social Sciences', 'Reference',\n",
              "       'Christian Books & Bibles', 'Religion & Spirituality',\n",
              "       'Sports & Outdoors', 'Teen & Young Adult', \"Children's Books\",\n",
              "       'Travel'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnduUu_mW2--"
      },
      "source": [
        "## Case Collapsing\n",
        "Change all uppercase to lowercase letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjC1iRCnW2--",
        "outputId": "1fcd1193-36d1-4ad0-ab41-d6789abbdc51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "case_collap = map(lambda x:x.lower(), titles1)\n",
        "case_collap_list = list(case_collap)\n",
        "print(case_collap_list[0:6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"mom's family wall calendar 2016\", 'doug the pug 2016 wall calendar', 'moleskine 2016 weekly notebook, 12m, large, black, soft cover (5 x 8.25)', '365 cats color page-a-day calendar 2016', 'sierra club engagement calendar 2016', 'sierra club wilderness calendar 2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyOSooAPW2--"
      },
      "source": [
        "## Remove Punctuation\n",
        "Remove punctuation by creating translation table\n",
        "Punctuation to be removed is given in string: string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGKeYu8fW2--",
        "outputId": "5fcc54af-db9b-4ee0-8499-94afaf99225d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "trans = str.maketrans('', '', string.punctuation)\n",
        "rem_punct = [s.translate(trans) for s in case_collap_list]\n",
        "print(rem_punct[0:6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['moms family wall calendar 2016', 'doug the pug 2016 wall calendar', 'moleskine 2016 weekly notebook 12m large black soft cover 5 x 825', '365 cats color pageaday calendar 2016', 'sierra club engagement calendar 2016', 'sierra club wilderness calendar 2016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODsYc112W2--"
      },
      "source": [
        "## Tokenization\n",
        "Split all titles into words\n",
        "Output: list of lists of strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uOP6Ni2b_Ur",
        "outputId": "906b5edb-69e6-4f5a-a8f2-dee854965431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnZV0ExRW2--",
        "outputId": "50d84088-5c0c-4d54-cccc-e9ce5a6e9b6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_titles = [word_tokenize(i) for i in rem_punct]\n",
        "print(tokenized_titles[:10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['moms', 'family', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskine', '2016', 'weekly', 'notebook', '12m', 'large', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cats', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engagement', 'calendar', '2016'], ['sierra', 'club', 'wilderness', 'calendar', '2016'], ['thomas', 'kinkade', 'the', 'disney', 'dreams', 'collection', '2016', 'wall', 'calendar'], ['ansel', 'adams', '2016', 'wall', 'calendar'], ['dilbert', '2016', 'daytoday', 'calendar'], ['mary', 'engelbreit', '2016', 'deluxe', 'wall', 'calendar', 'never', 'give', 'up']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMdT7PwpW2--"
      },
      "source": [
        "## N-Grams: Bigrams and Trigrams\n",
        "Create bigrams + trigrams\n",
        "(could be done in one go e.g. n=2 for bigrams, n=3 for trigrams etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UxYOxRFW2--",
        "outputId": "8e7463a4-66fa-43ea-a25d-e87793c2e4e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# bigrams\n",
        "token_bigram = []\n",
        "for title in tokenized_titles:\n",
        "    title_bigram = []\n",
        "    for w in range(len(title) - 1):\n",
        "        title_bigram.append([title[w], title[w + 1]])\n",
        "    token_bigram.append(title_bigram)\n",
        "print(token_bigram[:6]) # test whether working"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['moms', 'family'], ['family', 'wall'], ['wall', 'calendar'], ['calendar', '2016']], [['doug', 'the'], ['the', 'pug'], ['pug', '2016'], ['2016', 'wall'], ['wall', 'calendar']], [['moleskine', '2016'], ['2016', 'weekly'], ['weekly', 'notebook'], ['notebook', '12m'], ['12m', 'large'], ['large', 'black'], ['black', 'soft'], ['soft', 'cover'], ['cover', '5'], ['5', 'x'], ['x', '825']], [['365', 'cats'], ['cats', 'color'], ['color', 'pageaday'], ['pageaday', 'calendar'], ['calendar', '2016']], [['sierra', 'club'], ['club', 'engagement'], ['engagement', 'calendar'], ['calendar', '2016']], [['sierra', 'club'], ['club', 'wilderness'], ['wilderness', 'calendar'], ['calendar', '2016']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvpg0DUjW2--",
        "outputId": "c5971641-21ea-4ef7-9f39-9e2cd9f558f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# trigrams\n",
        "token_trigram = []\n",
        "for title in tokenized_titles:\n",
        "    title_trigram = []\n",
        "    for w in range(len(title) - 2):\n",
        "        title_trigram.append([title[w], title[w + 1], title[w + 2]])\n",
        "    token_trigram.append(title_trigram)\n",
        "print(token_trigram[:6]) # test whether working"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['moms', 'family', 'wall'], ['family', 'wall', 'calendar'], ['wall', 'calendar', '2016']], [['doug', 'the', 'pug'], ['the', 'pug', '2016'], ['pug', '2016', 'wall'], ['2016', 'wall', 'calendar']], [['moleskine', '2016', 'weekly'], ['2016', 'weekly', 'notebook'], ['weekly', 'notebook', '12m'], ['notebook', '12m', 'large'], ['12m', 'large', 'black'], ['large', 'black', 'soft'], ['black', 'soft', 'cover'], ['soft', 'cover', '5'], ['cover', '5', 'x'], ['5', 'x', '825']], [['365', 'cats', 'color'], ['cats', 'color', 'pageaday'], ['color', 'pageaday', 'calendar'], ['pageaday', 'calendar', '2016']], [['sierra', 'club', 'engagement'], ['club', 'engagement', 'calendar'], ['engagement', 'calendar', '2016']], [['sierra', 'club', 'wilderness'], ['club', 'wilderness', 'calendar'], ['wilderness', 'calendar', '2016']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7o_3m6W2-_"
      },
      "source": [
        "## Stemming\n",
        "Test this out to see whether it makes a difference in final classifier\n",
        "\n",
        "PorterStemmer (one algorithm for stemming; less aggressive than LancasterStemming)\n",
        "\n",
        "Create empty list to contain lists of stems in each title\n",
        "Create empty list for stems of title\n",
        "Add each stemmed word in title to second list\n",
        "Append list of stemmed words to first list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFbx1Yc8W2-_",
        "outputId": "df9ca26a-5f94-4978-9911-07cbdc50fd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "stems = []   \n",
        "for title in tokenized_titles:\n",
        "    stems_title = []\n",
        "    for word in title:\n",
        "        stems_title.append(porter.stem(word))\n",
        "    stems.append(stems_title)\n",
        "    \n",
        "print(stems[0:6]) # test whether it works"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mom', 'famili', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskin', '2016', 'weekli', 'notebook', '12m', 'larg', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cat', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engag', 'calendar', '2016'], ['sierra', 'club', 'wilder', 'calendar', '2016']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoQP5yfIW2-_"
      },
      "source": [
        "## Lemmatization\n",
        "Same principle as with stemming\n",
        "Test this out to see whether it makes a difference in final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dDPjCeW2-_",
        "outputId": "4821fa24-874b-4e6f-c135-6c35609767ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = []\n",
        "for title in tokenized_titles:\n",
        "    lemmas_title = []\n",
        "    for word in title:\n",
        "        lemmas_title.append(lemmatizer.lemmatize(word))\n",
        "    lemmas.append(lemmas_title)\n",
        "    \n",
        "print(lemmas[0:6]) # test whether it works"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[['mom', 'family', 'wall', 'calendar', '2016'], ['doug', 'the', 'pug', '2016', 'wall', 'calendar'], ['moleskine', '2016', 'weekly', 'notebook', '12m', 'large', 'black', 'soft', 'cover', '5', 'x', '825'], ['365', 'cat', 'color', 'pageaday', 'calendar', '2016'], ['sierra', 'club', 'engagement', 'calendar', '2016'], ['sierra', 'club', 'wilderness', 'calendar', '2016']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUsMyJasW2-_"
      },
      "source": [
        "## Part-of-Speech (POS) Tagging\n",
        "Create list of lists with tokens and their corresponding part-of-speech tag in each title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MJ-Wmu2xW2-_",
        "outputId": "f19fc8ce-8e6e-45c7-94d0-8e6dbe3d31d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "postag = []\n",
        "for title in tokenized_titles:\n",
        "    postag.append(nltk.pos_tag(title))\n",
        "    \n",
        "print(postag[0:6]) # testing whether postag worked"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[[('moms', 'NNS'), ('family', 'NN'), ('wall', 'NN'), ('calendar', 'NN'), ('2016', 'CD')], [('doug', 'VB'), ('the', 'DT'), ('pug', 'NN'), ('2016', 'CD'), ('wall', 'NN'), ('calendar', 'NN')], [('moleskine', 'NN'), ('2016', 'CD'), ('weekly', 'JJ'), ('notebook', 'NN'), ('12m', 'CD'), ('large', 'JJ'), ('black', 'JJ'), ('soft', 'JJ'), ('cover', 'NN'), ('5', 'CD'), ('x', 'JJ'), ('825', 'CD')], [('365', 'CD'), ('cats', 'NNS'), ('color', 'VBP'), ('pageaday', 'IN'), ('calendar', 'NN'), ('2016', 'CD')], [('sierra', 'NN'), ('club', 'NN'), ('engagement', 'NN'), ('calendar', 'NN'), ('2016', 'CD')], [('sierra', 'NN'), ('club', 'NN'), ('wilderness', 'NN'), ('calendar', 'NN'), ('2016', 'CD')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n12VN7eW2-_"
      },
      "source": [
        "## Named Entity Recognition (NER)\n",
        "Create list of lists with words, correspondent POS and named entity tags for each title\n",
        "This uses postags created in previous step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PR46UHjDW2-_",
        "outputId": "8b316ced-a3d7-4ede-ab5c-16120f76a396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "\n",
        "ner = []\n",
        "for title in titles1:\n",
        "    ner.append(tree2conlltags(ne_chunk(pos_tag(word_tokenize(title)))))\n",
        "    \n",
        "print(ner[0:6]) # test whether NER worked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jEKmcN8W2-_"
      },
      "source": [
        "# Classifiers\n",
        "Check whether stemming/lemmatization make difference in final classifier\n",
        "-> Does it improve/worsen classifier?\n",
        "\n",
        "Naive Bayes, BERT, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70fzliicdQwQ"
      },
      "source": [
        "# create dataframe containing tokenized titles and genres\n",
        "tok_title = pd.DataFrame({0: tokenized_titles})\n",
        "data_in = [tok_title[0], df1[\"genre\"]]\n",
        "headers = [\"titles\", \"genres\"]\n",
        "\n",
        "data = pd.concat(data_in, axis=1, keys=headers)\n",
        "print(data[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znLw2BDMdQVh"
      },
      "source": [
        "# split data into train and test\n",
        "import numpy as np\n",
        "\n",
        "test_pct=0.2 # split into 80/20%\n",
        "\n",
        "# create mask\n",
        "mask = np.random.choice([0, 1], p=[1 - test_pct, test_pct], size=data.shape[0])\n",
        "\n",
        "# apply mask\n",
        "data[\"mask\"] = mask\n",
        "test = data[data[\"mask\"] == 1]\n",
        "train = data[data[\"mask\"] == 0]\n",
        "\n",
        "# removing column\n",
        "test = test.drop(\"mask\", axis=\"columns\").reset_index()\n",
        "train = train.drop(\"mask\", axis=\"columns\").reset_index()\n",
        "\n",
        "# remove original indexing data (otherwise we have double indexing)\n",
        "test = test.drop(\"index\", axis=\"columns\")\n",
        "train = train.drop(\"index\", axis=\"columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK2D0LTOW2-_"
      },
      "source": [
        "## Naive Bayes classifier\n",
        "\n",
        "split into training and test data -> 80% / 20%\n",
        "use n-grams ?\n",
        "\n",
        "build vocabulary -> need token frequencies across all titles (create sublist with tokenized_titles)\n",
        "vocabulary for each genre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqjP4pQSdJiz"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Sn1vYQdJ8t"
      },
      "source": [
        "# token frequencies within each title\n",
        "unigram_count = []\n",
        "for title in tokenized_titles:\n",
        "    uni_title = Counter()\n",
        "    for i in title:\n",
        "        uni_title[i] += 1\n",
        "    unigram_count.append(uni_title)\n",
        "\n",
        "print(unigram_count[:5]) # test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDYntLiAdKTo"
      },
      "source": [
        "# token frequencies in total vocab\n",
        "tok_freq = Counter()\n",
        "for title in train['titles']:\n",
        "    for i in title:\n",
        "        tok_freq[i] += 1\n",
        "        \n",
        "print(tok_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbfTBYhFdYI-"
      },
      "source": [
        "# group data by genre\n",
        "\n",
        "grouped = train.groupby(train.genres)\n",
        "\n",
        "Calendars = grouped.get_group(\"Calendars\")\n",
        "Comics = grouped.get_group(\"Comics & Graphic Novels\")\n",
        "Test = grouped.get_group(\"Test Preparation\")\n",
        "Mystery = grouped.get_group(\"Mystery, Thriller & Suspense\")\n",
        "SciFi = grouped.get_group(\"Science Fiction & Fantasy\")\n",
        "Romance = grouped.get_group(\"Romance\")\n",
        "Humor = grouped.get_group(\"Humor & Entertainment\")\n",
        "Literature = grouped.get_group(\"Literature & Fiction\")\n",
        "LGBTQ = grouped.get_group(\"Gay & Lesbian\")\n",
        "Engineering = grouped.get_group(\"Engineering & Transportation\")\n",
        "Food = grouped.get_group(\"Cookbooks, Food & Wine\")\n",
        "Crafts = grouped.get_group(\"Crafts, Hobbies & Home\")\n",
        "Arts = grouped.get_group(\"Arts & Photography\")\n",
        "Education = grouped.get_group(\"Education & Teaching\")\n",
        "Parenting = grouped.get_group(\"Parenting & Relationships\")\n",
        "SelfHelp = grouped.get_group(\"Self-Help\")\n",
        "Computers = grouped.get_group(\"Computers & Technology\")\n",
        "Medical = grouped.get_group(\"Medical Books\")\n",
        "Science = grouped.get_group(\"Science & Math\")\n",
        "Health = grouped.get_group(\"Health, Fitness & Dieting\")\n",
        "Business = grouped.get_group(\"Business & Money\")\n",
        "Law = grouped.get_group(\"Law\")\n",
        "Biographies = grouped.get_group(\"Biographies & Memoirs\")\n",
        "History = grouped.get_group(\"History\")\n",
        "Politics = grouped.get_group(\"Politics & Social Sciences\")\n",
        "Reference = grouped.get_group(\"Reference\")\n",
        "Bibles = grouped.get_group(\"Christian Books & Bibles\")\n",
        "Religion = grouped.get_group(\"Religion & Spirituality\")\n",
        "Sports = grouped.get_group(\"Sports & Outdoors\")\n",
        "Teen = grouped.get_group(\"Teen & Young Adult\")\n",
        "Childrens = grouped.get_group(\"Children's Books\")\n",
        "Travel = grouped.get_group(\"Travel\")\n",
        "\n",
        "GenreGroups = [Calendars['titles'], Comics['titles'], Test['titles'], Mystery['titles'], SciFi['titles'], \n",
        "               Romance['titles'], Humor['titles'], Literature['titles'], LGBTQ['titles'], Engineering['titles'], \n",
        "               Food['titles'], Crafts['titles'], Arts['titles'], Education['titles'], Parenting['titles'], \n",
        "               SelfHelp['titles'], Computers['titles'], Medical['titles'], Science['titles'], Health['titles'], \n",
        "               Business['titles'], Law['titles'], Biographies['titles'], History['titles'], Politics['titles'], \n",
        "               Reference['titles'], Bibles['titles'], Religion['titles'], Sports['titles'], Teen['titles'], \n",
        "               Childrens['titles'], Travel['titles']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18-C-OtCdYHf"
      },
      "source": [
        "# token frequencies in each genre\n",
        "keys = ['Calendars', 'Comics & Graphic Novels', 'Test Preparation',\n",
        "       'Mystery, Thriller & Suspense', 'Science Fiction & Fantasy',\n",
        "       'Romance', 'Humor & Entertainment', 'Literature & Fiction',\n",
        "       'Gay & Lesbian', 'Engineering & Transportation',\n",
        "       'Cookbooks, Food & Wine', 'Crafts, Hobbies & Home',\n",
        "       'Arts & Photography', 'Education & Teaching',\n",
        "       'Parenting & Relationships', 'Self-Help', 'Computers & Technology',\n",
        "       'Medical Books', 'Science & Math', 'Health, Fitness & Dieting',\n",
        "       'Business & Money', 'Law', 'Biographies & Memoirs', 'History',\n",
        "       'Politics & Social Sciences', 'Reference',\n",
        "       'Christian Books & Bibles', 'Religion & Spirituality',\n",
        "       'Sports & Outdoors', 'Teen & Young Adult', \"Children's Books\",\n",
        "       'Travel']\n",
        "\n",
        "genre_count = []\n",
        "for g in GenreGroups:\n",
        "    genre_title = Counter()\n",
        "    for title in g:\n",
        "        for i in title:\n",
        "            genre_title[i] += 1\n",
        "    genre_count.append(genre_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-g5qVgpdYEf"
      },
      "source": [
        "# combine keys and titles grouped by genres\n",
        "zipped_values = zip(keys, genre_count)\n",
        "tok_freq_genres = list(zipped_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyC56_LFdYB7"
      },
      "source": [
        "tok_freq_genres[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpFvXnUOdYAD"
      },
      "source": [
        "# number of words in a class\n",
        "len(tok_freq_genres[0][1]) # first genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC9GeuomdX85"
      },
      "source": [
        "# class name\n",
        "tok_freq_genres[0][0] # first genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPgWxf-AdX20"
      },
      "source": [
        "# number of total vocabulary (training set)\n",
        "V = len(Counter(tok_freq))\n",
        "print(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zKN8UtBdXva"
      },
      "source": [
        "# number of titles (in training set)\n",
        "N_titles = len(train)\n",
        "print(N_titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2akqSt0IdXml"
      },
      "source": [
        "# number of titles in each genre\n",
        "N_genre = train['genres'].value_counts()\n",
        "print(N_genre[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmz-NgqQdmzv"
      },
      "source": [
        "N_genre['Travel'] # access number of titles in specified genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAJc0aVGdmwG"
      },
      "source": [
        "# compute priors for each class\n",
        "# total vocab = V\n",
        "# number of words in each class -> token frequencies for each genre\n",
        "\n",
        "for i in train['genres']:\n",
        "    # count words within each document for all documents in class\n",
        "    \n",
        "# probabilities of title being in given genre\n",
        "probabilities = []\n",
        "for i in N_genre:\n",
        "    probabilities = i / N_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnYHYneEdmuU"
      },
      "source": [
        "# likelihoods for each word\n",
        "# probability genre given word\n",
        "# x = token frequencies for each genre -> saved as dictionary in tok_freq_genres[0][1] for first genre\n",
        "\n",
        "# Nc = number of words in class\n",
        "for i in tok_freq_genres[0][i]:\n",
        "    len(tok_freq_genres[0][i]) # gives length of i-th genre\n",
        "tok_freq_genres[0][0] # gives name of first genre\n",
        "# V = total vocab\n",
        "\n",
        "#(x+1) / (Nc + V)\n",
        "\n",
        "# probability genre given word not in testset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2WRs7P_dmsB"
      },
      "source": [
        "# test NB using test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyE3csOxdmqR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzho5snTW2-_"
      },
      "source": [
        "## BERT classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmtDDdmNdv2N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}